"""
æ–‡æ¡£å¤„ç†æ¨¡å—: ä»PDFæå–æ–‡æœ¬å¹¶æ™ºèƒ½åˆ‡åˆ†
"""
import re
from typing import List, Dict
import PyPDF2


class DocumentProcessor:
    def __init__(self, chunk_size=500, overlap=50):
        """
        Args:
            chunk_size: æ¯ä¸ªchunkçš„ç›®æ ‡å­—ç¬¦æ•°
            overlap: chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°(ä¿è¯ä¸Šä¸‹æ–‡è¿è´¯)
        """
        self.chunk_size = chunk_size
        self.overlap = overlap

    def load_pdf(self, pdf_path: str) -> str:
        """
        ä»PDFæå–æ–‡æœ¬

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        Returns:
            æå–çš„å…¨éƒ¨æ–‡æœ¬
        """
        print(f"ğŸ“„ æ­£åœ¨è¯»å–PDF: {pdf_path}")

        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)

                print(f"ğŸ“– æ€»é¡µæ•°: {total_pages}")

                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    text += page_text
                    print(f"  âœ“ å·²å¤„ç† {page_num}/{total_pages} é¡µ")

            print(f"âœ… PDFè¯»å–å®Œæˆ,å…± {len(text)} å­—ç¬¦\n")
            return text

        except Exception as e:
            print(f"âŒ è¯»å–PDFå¤±è´¥: {e}")
            return ""

    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬(å»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç­‰)
        """
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤ç‰¹æ®Šå­—ç¬¦(æ ¹æ®éœ€è¦è°ƒæ•´)
        text = text.strip()
        return text

    def chunk_by_sentences(self, text: str) -> List[str]:
        """
        æŒ‰å¥å­åˆ‡åˆ†æ–‡æœ¬(æ™ºèƒ½æ–¹æ³•)

        ç­–ç•¥:
        1. å…ˆæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ‡åˆ†æˆå¥å­
        2. æŠŠå¥å­ç»„åˆæˆchunk,ä¿æŒåœ¨chunk_sizeå·¦å³
        3. chunkä¹‹é—´æœ‰overlap,ä¿è¯ä¸Šä¸‹æ–‡è¿è´¯
        """
        print(f"ğŸ”ª å¼€å§‹åˆ‡åˆ†æ–‡æœ¬...")
        print(f"   ç›®æ ‡chunkå¤§å°: {self.chunk_size} å­—ç¬¦")
        print(f"   é‡å åŒºåŸŸ: {self.overlap} å­—ç¬¦\n")

        # æŒ‰å¥å­åˆ‡åˆ†(ä¸­è‹±æ–‡æ ‡ç‚¹)
        sentences = re.split(r'([ã€‚!?\.!\?])', text)
        # æŠŠæ ‡ç‚¹ç¬¦å·é™„åŠ å›å»
        sentences_with_punct = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                sentences_with_punct.append(sentences[i] + sentences[i + 1])

        # ç»„åˆæˆchunks
        chunks = []
        current_chunk = ""

        for sentence in sentences_with_punct:
            # å¦‚æœåŠ ä¸Šè¿™å¥è¯ä¼šè¶…è¿‡chunk_size
            if len(current_chunk) + len(sentence) > self.chunk_size:
                if current_chunk:  # ä¿å­˜å½“å‰chunk
                    chunks.append(current_chunk.strip())
                    # ä¿ç•™overlapéƒ¨åˆ†ä½œä¸ºä¸‹ä¸€ä¸ªchunkçš„å¼€å¤´
                    current_chunk = current_chunk[-self.overlap:] + sentence
                else:
                    current_chunk = sentence
            else:
                current_chunk += sentence

        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk.strip())

        print(f"âœ… åˆ‡åˆ†å®Œæˆ! å…± {len(chunks)} ä¸ªchunks")
        print(f"   å¹³å‡chunké•¿åº¦: {sum(len(c) for c in chunks) // len(chunks)} å­—ç¬¦\n")

        return chunks

    def chunk_by_fixed_size(self, text: str) -> List[str]:
        """
        æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†(ç®€å•æ–¹æ³•)
        """
        chunks = []
        start = 0

        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - self.overlap  # åé€€overlap,ä¿è¯é‡å 

        return chunks

    def process_pdf(self, pdf_path: str, method='sentences') -> List[Dict]:
        """
        å®Œæ•´å¤„ç†æµç¨‹: PDF â†’ chunks

        Args:
            pdf_path: PDFè·¯å¾„
            method: 'sentences' æˆ– 'fixed'
        Returns:
            List of {text: str, metadata: dict}
        """
        # 1. è¯»å–PDF
        raw_text = self.load_pdf(pdf_path)

        if not raw_text:
            return []

        # 2. æ¸…ç†æ–‡æœ¬
        cleaned_text = self.clean_text(raw_text)

        # 3. åˆ‡åˆ†
        if method == 'sentences':
            chunks = self.chunk_by_sentences(cleaned_text)
        else:
            chunks = self.chunk_by_fixed_size(cleaned_text)

        # 4. æ·»åŠ metadata
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            processed_chunks.append({
                'text': chunk,
                'metadata': {
                    'source': pdf_path,
                    'chunk_id': i,
                    'total_chunks': len(chunks)
                }
            })

        return processed_chunks


# ========== æµ‹è¯•ä»£ç  ==========
def demo():
    """
    æ¼”ç¤ºæ–‡æ¡£å¤„ç†åŠŸèƒ½
    """
    print("=" * 60)
    print("ğŸ“š æ–‡æ¡£å¤„ç†å™¨æ¼”ç¤º")
    print("=" * 60)
    print()

    # åˆ›å»ºå¤„ç†å™¨
    processor = DocumentProcessor(chunk_size=300, overlap=50)

    # æµ‹è¯•1: å¤„ç†ç¤ºä¾‹æ–‡æœ¬
    print("ğŸ§ª æµ‹è¯•1: å¤„ç†ç¤ºä¾‹æ–‡æœ¬")
    print("-" * 60)

    sample_text = """
    å¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚
    å®ƒé€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒ,å­¦ä¹ åˆ°ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚
    ç›®å‰ä¸»æµçš„LLMåŒ…æ‹¬GPTç³»åˆ—ã€BERTã€LLaMAç­‰ã€‚
    è¿™äº›æ¨¡å‹åœ¨é—®ç­”ã€ç¿»è¯‘ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚

    RAG(Retrieval-Augmented Generation)æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
    å®ƒé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£,ä¸ºLLMæä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
    è¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å¹»è§‰é—®é¢˜,æé«˜å›ç­”çš„å‡†ç¡®æ€§ã€‚
    RAGç³»ç»Ÿé€šå¸¸åŒ…æ‹¬æ–‡æ¡£å¤„ç†ã€å‘é‡æ£€ç´¢ã€æç¤ºå·¥ç¨‹ç­‰æ¨¡å—ã€‚
    """

    chunks = processor.chunk_by_sentences(sample_text)

    print("ğŸ“‹ åˆ‡åˆ†ç»“æœ:")
    for i, chunk in enumerate(chunks):
        print(f"\nChunk {i + 1}:")
        print(f"  é•¿åº¦: {len(chunk)} å­—ç¬¦")
        print(f"  å†…å®¹: {chunk[:100]}...")

    # æµ‹è¯•2: å¦‚æœä½ æœ‰PDFæ–‡ä»¶
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•2: å¤„ç†PDFæ–‡ä»¶(å¯é€‰)")
    print("-" * 60)
    print("æç¤º: æŠŠä½ çš„PDFæ–‡ä»¶æ”¾åœ¨é¡¹ç›®æ–‡ä»¶å¤¹,ç„¶åå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š\n")

    # å–æ¶ˆæ³¨é‡Šæ¥æµ‹è¯•PDFå¤„ç†:
    pdf_path = "3.3.pdf"
    chunks = processor.process_pdf(pdf_path, method='sentences')

    print(f"âœ… å¤„ç†å®Œæˆ!")
    print(f"   æ€»chunksæ•°: {len(chunks)}")
    print(f"\nå‰3ä¸ªchunks:")
    for chunk_data in chunks[:3]:
      print(f"\n  Chunk {chunk_data['metadata']['chunk_id']}:")
      print(f"    {chunk_data['text'][:150]}...")


if __name__ == "__main__":
    demo()