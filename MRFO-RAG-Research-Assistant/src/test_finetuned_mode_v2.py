"""
æµ‹è¯•å¾®è°ƒåçš„MRFOä¸“å®¶æ¨¡å‹ - æ”¹è¿›ç‰ˆ
å¯ä»¥é€‰æ‹©æµ‹è¯•ä¸åŒç‰ˆæœ¬çš„å¾®è°ƒæ¨¡å‹
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os


class FinetunedModelTester:
    def __init__(self, lora_path):
        print("=" * 70)
        print("ğŸ§ª åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
        print("=" * 70)
        print(f"ğŸ“ LoRAè·¯å¾„: {lora_path}")

        base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"

        # 1. åŠ è½½tokenizer
        print("\nğŸ”„ åŠ è½½Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            trust_remote_code=True
        )

        # 2. åŠ è½½åŸºç¡€æ¨¡å‹(4bit)
        print("ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
        from transformers import BitsAndBytesConfig

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )

        # 3. åŠ è½½LoRAæƒé‡
        print("ğŸ”„ åŠ è½½LoRAå¾®è°ƒæƒé‡...")
        self.model = PeftModel.from_pretrained(base_model, lora_path)

        print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ!")
        print("=" * 70)

    def generate(self, question: str, max_new_tokens: int = 256) -> str:
        """ç”Ÿæˆå›ç­”"""
        messages = [
            {"role": "user", "content": question}
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        )

        return response


def test_critical_questions(model):
    """
    æµ‹è¯•å…³é”®é—®é¢˜(ä¹‹å‰å®¹æ˜“å‡ºé”™çš„)
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ å…³é”®é—®é¢˜æµ‹è¯•")
    print("=" * 70)

    critical_tests = [
        {
            "question": "ä»€ä¹ˆæ˜¯MRFOç®—æ³•?",
            "check_keywords": ["è é²¼", "Manta Ray", "è§…é£Ÿä¼˜åŒ–"],
            "avoid_keywords": ["Multi-Objective"],
            "note": "æ£€æŸ¥æ˜¯å¦æ­£ç¡®ç†è§£MRFO"
        },
        {
            "question": "åœ¨å¤æ‚åœºæ™¯ä¸‹,DLM MRFOç›¸æ¯”MRFOç®—æ³•é™ä½äº†å¤šå°‘æˆæœ¬?",
            "check_keywords": ["5.89%"],
            "avoid_keywords": ["53.29%", "7.89%"],
            "note": "æ£€æŸ¥æ•°å­—å‡†ç¡®æ€§"
        },
        {
            "question": "MRFOç®—æ³•çš„ä¸‰ç§è§…é£Ÿç­–ç•¥æ˜¯ä»€ä¹ˆ?",
            "check_keywords": ["é“¾å¼", "èºæ—‹", "ç¿»æ»š"],
            "avoid_keywords": [],
            "note": "æ£€æŸ¥åŸºç¡€æ¦‚å¿µ"
        },
        {
            "question": "DLM MRFOå¼•å…¥äº†å“ªäº›æ”¹è¿›æœºåˆ¶?",
            "check_keywords": ["ç¦»æ•£", "åŠ¨æ€æƒé‡", "é•¿æ—¶è®°å¿†", "å˜å¼‚", "PAR"],
            "avoid_keywords": [],
            "note": "æ£€æŸ¥å¤šç‚¹è®°å¿†(è‡³å°‘åŒ…å«3ä¸ª)"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯å³°å€¼å¹³å‡æ¯”PAR?",
            "check_keywords": ["Peak-to-Average", "å³°å€¼è´Ÿè½½", "å¹³å‡è´Ÿè½½"],
            "avoid_keywords": [],
            "note": "æ£€æŸ¥æœ¯è¯­ç†è§£"
        }
    ]

    total_score = 0
    max_score = 0

    for i, test in enumerate(critical_tests, 1):
        print(f"\n{'=' * 70}")
        print(f"æµ‹è¯• {i}/{len(critical_tests)}: {test['note']}")
        print(f"{'=' * 70}")
        print(f"\nâ“ é—®é¢˜: {test['question']}")
        print("-" * 70)

        answer = model.generate(test['question'], max_new_tokens=250)
        print(f"\nğŸ’¡ å›ç­”:\n{answer}")

        # è¯„åˆ†
        score = 0
        check_count = 0

        print(f"\nğŸ“Š è¯„ä¼°:")

        # æ£€æŸ¥å¿…é¡»åŒ…å«çš„å…³é”®è¯
        for keyword in test['check_keywords']:
            if keyword in answer:
                print(f"  âœ… åŒ…å«å…³é”®è¯: {keyword}")
                score += 1
            else:
                print(f"  âŒ ç¼ºå°‘å…³é”®è¯: {keyword}")
            check_count += 1

        # æ£€æŸ¥ä¸åº”åŒ…å«çš„å…³é”®è¯
        for keyword in test['avoid_keywords']:
            if keyword in answer:
                print(f"  âš ï¸  åŒ…å«é”™è¯¯å†…å®¹: {keyword}")
                score -= 0.5
            check_count += 0.5

        total_score += score
        max_score += len(test['check_keywords'])

        print(f"\nå¾—åˆ†: {score}/{len(test['check_keywords'])}")
        print("-" * 70)
        input("æŒ‰Enterç»§ç»­...")

    print("\n" + "=" * 70)
    print("ğŸ“Š æ€»ä½“è¯„åˆ†")
    print("=" * 70)
    accuracy = (total_score / max_score * 100) if max_score > 0 else 0
    print(f"âœ… å‡†ç¡®ç‡: {accuracy:.1f}% ({total_score:.1f}/{max_score})")

    if accuracy >= 80:
        print("ğŸ‰ ä¼˜ç§€! å¾®è°ƒæ•ˆæœå¾ˆå¥½!")
    elif accuracy >= 60:
        print("ğŸ‘ è‰¯å¥½,ä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("âš ï¸  æ•ˆæœä¸å¤Ÿç†æƒ³,å»ºè®®ç”¨æ›´æ¿€è¿›çš„é…ç½®é‡æ–°è®­ç»ƒ")

    return accuracy


def main():
    print("ğŸ“ å¾®è°ƒæ¨¡å‹æµ‹è¯•ç¨‹åº v2.0")
    print()

    # 1. é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ç‰ˆæœ¬
    print("å¯ç”¨çš„å¾®è°ƒæ¨¡å‹:")
    models = []

    if os.path.exists("./saves/mrfo_lora_quick"):
        models.append(("quick", "./saves/mrfo_lora_quick"))
        print("  1. å¿«é€Ÿæ”¹è¿›ç‰ˆ")

    if os.path.exists("./saves/mrfo_lora_balanced"):
        models.append(("balanced", "./saves/mrfo_lora_balanced"))
        print("  2. å¹³è¡¡æ”¹è¿›ç‰ˆ")

    if os.path.exists("./saves/mrfo_lora_aggressive"):
        models.append(("aggressive", "./saves/mrfo_lora_aggressive"))
        print("  3. æ¿€è¿›æ”¹è¿›ç‰ˆ")

    if os.path.exists("./saves/mrfo_lora"):
        models.append(("original", "./saves/mrfo_lora"))
        print("  4. åŸå§‹ç‰ˆæœ¬(ç¬¬ä¸€æ¬¡è®­ç»ƒ)")

    if not models:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¾®è°ƒæ¨¡å‹!")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒ: python run_training_v2.py")
        return

    print()
    choice = input(f"é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ (1-{len(models)}): ").strip()

    try:
        idx = int(choice) - 1
        model_name, model_path = models[idx]
    except:
        print("æ— æ•ˆé€‰æ‹©,ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹")
        model_name, model_path = models[-1]

    print(f"\nâœ… å°†æµ‹è¯•: {model_name} ({model_path})")

    # 2. åŠ è½½æ¨¡å‹
    model = FinetunedModelTester(model_path)

    # 3. è¿è¡Œæµ‹è¯•
    accuracy = test_critical_questions(model)

    # 4. å»ºè®®
    print("\n" + "=" * 70)
    print("ğŸ’¡ æ”¹è¿›å»ºè®®")
    print("=" * 70)

    if accuracy < 60:
        print("\næ•ˆæœä¸å¤Ÿç†æƒ³,å»ºè®®:")
        print("  1. ä½¿ç”¨æ›´æ¿€è¿›çš„é…ç½®é‡æ–°è®­ç»ƒ")
        print("     python run_training_v2.py")
        print("     é€‰æ‹© '3. æ¿€è¿›æ”¹è¿›'")
        print()
        print("  2. æˆ–è€…å¢åŠ è®­ç»ƒæ•°æ®")
        print("     ä»è®ºæ–‡ä¸­æå–æ›´å¤šQAå¯¹")
    elif accuracy < 80:
        print("\næ•ˆæœè‰¯å¥½ä½†å¯ä»¥æ›´å¥½,å»ºè®®:")
        print("  - å¦‚æœç”¨çš„æ˜¯'å¿«é€Ÿ'é…ç½®,è¯•è¯•'å¹³è¡¡'æˆ–'æ¿€è¿›'")
        print("  - æ£€æŸ¥è®­ç»ƒLossæ˜¯å¦å……åˆ†ä¸‹é™(< 1.0)")
    else:
        print("\nâœ… å¾®è°ƒæ•ˆæœå¾ˆå¥½!")
        print("å¯ä»¥è¿›å…¥ä¸‹ä¸€æ­¥: æ•´åˆåˆ°RAGç³»ç»Ÿ")


if __name__ == "__main__":
    main()