"""
æ”¹è¿›ç‰ˆRAGç³»ç»Ÿ - ä¿®å¤ç›¸ä¼¼åº¦è®¡ç®—å’Œå‡å°‘å¹»è§‰
"""
import os
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

from document_processor import DocumentProcessor
from local_llm import LocalLLM


class AdvancedRAGv2:
    def __init__(
            self,
            embedding_model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            llm_model_name: str = "Qwen/Qwen2.5-1.5B-Instruct",
            collection_name: str = "research_knowledge_base_v2"
    ):
        """
        åˆå§‹åŒ–æ”¹è¿›ç‰ˆRAGç³»ç»Ÿ
        """
        print("=" * 70)
        print("ğŸš€ åˆå§‹åŒ–æ”¹è¿›ç‰ˆRAGç³»ç»Ÿ v2.0")
        print("=" * 70)

        # 1. åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨(æ›´å°çš„chunk)
        print("\nğŸ“„ åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨...")
        self.doc_processor = DocumentProcessor(
            chunk_size=300,  # â¬…ï¸ æ”¹å°
            overlap=80  # â¬…ï¸ å¢åŠ 
        )
        print("âœ… æ–‡æ¡£å¤„ç†å™¨å°±ç»ª(chunk_size=300, overlap=80)")

        # 2. åˆå§‹åŒ–Embeddingæ¨¡å‹
        print("\nğŸ”„ åŠ è½½Embeddingæ¨¡å‹...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        print("âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")

        # 3. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        print("\nğŸ’¾ åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))

        try:
            self.collection = self.chroma_client.get_collection(name=collection_name)
            print(f"âœ… å·²åŠ è½½ç°æœ‰çŸ¥è¯†åº“: {collection_name}")
        except:
            self.collection = self.chroma_client.create_collection(name=collection_name)
            print(f"âœ… å·²åˆ›å»ºæ–°çŸ¥è¯†åº“: {collection_name}")

        # 4. åˆå§‹åŒ–LLM
        print("\nğŸ¤– åˆå§‹åŒ–æœ¬åœ°LLM...")
        self.llm = LocalLLM(
            model_name=llm_model_name,
            use_4bit=True
        )

        print("\n" + "=" * 70)
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
        print("=" * 70)
        print()

    def add_documents_from_pdf(self, pdf_path: str) -> int:
        """ä»PDFæ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        print(f"\nğŸ“š æ­£åœ¨å¤„ç†PDF: {pdf_path}")
        print("-" * 70)

        chunks = self.doc_processor.process_pdf(pdf_path, method='sentences')

        if not chunks:
            print("âŒ PDFå¤„ç†å¤±è´¥")
            return 0

        print(f"\nğŸ”„ æ­£åœ¨ä¸º {len(chunks)} ä¸ªchunksç”Ÿæˆå‘é‡...")

        for i, chunk_data in enumerate(chunks):
            text = chunk_data['text']
            metadata = chunk_data['metadata']

            embedding = self.embedding_model.encode(text).tolist()
            doc_id = f"{os.path.basename(pdf_path)}_chunk_{i}"

            self.collection.add(
                embeddings=[embedding],
                documents=[text],
                metadatas=[metadata],
                ids=[doc_id]
            )

            if (i + 1) % 10 == 0:
                print(f"  å·²å¤„ç†: {i + 1}/{len(chunks)} chunks")

        print(f"\nâœ… æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªchunksåˆ°çŸ¥è¯†åº“!")
        return len(chunks)

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£(ä¿®å¤äº†ç›¸ä¼¼åº¦è®¡ç®—)
        """
        query_embedding = self.embedding_model.encode(query).tolist()

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        retrieved_docs = []
        for i in range(len(results['documents'][0])):
            distance = results['distances'][0][i] if results['distances'] else 0

            # â¬‡ï¸ ä¿®å¤: æ­£ç¡®è®¡ç®—ç›¸ä¼¼åº¦
            similarity = 1 / (1 + distance)

            retrieved_docs.append({
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                'similarity': similarity,
                'distance': distance
            })

        return retrieved_docs

    def build_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:
        """
        æ„å»ºä¸¥æ ¼çš„prompt,å‡å°‘å¹»è§‰
        """
        context = "\n\n".join([
            f"ã€å‚è€ƒèµ„æ–™{i + 1}ã€‘\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])

        # â¬‡ï¸ æ›´ä¸¥æ ¼çš„prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç ”ç©¶åŠ©æ‰‹ã€‚è¯·**ä¸¥æ ¼**æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€é‡è¦è§„åˆ™ã€‘
1. **åªèƒ½**ä½¿ç”¨ä¸Šè¿°å‚è€ƒèµ„æ–™ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯
2. **ç¦æ­¢**æ·»åŠ ä»»ä½•å‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰çš„å†…å®¹
3. **ç¦æ­¢**æ¨æµ‹ã€çŒœæµ‹æˆ–è”æƒ³
4. å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³,å¿…é¡»è¯´"å‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜"
5. ç›´æ¥å¼•ç”¨å…³é”®åŸæ–‡,ç”¨è‡ªå·±çš„è¯ç®€æ´æ€»ç»“
6.ä¸ç¡®å®šçš„å†…å®¹è¯·ç›´æ¥å¼•ç”¨åŸæ–‡ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå…·ä½“æ•°å­—çš„å›ç­”ï¼Œä¸èƒ½è‡ªå·±èƒ¡ä¹±å¡«å†™æˆ–è€…æ··æ·†æ•°æ®
7.æœ¯è¯­è¯·ä¿è¯ä¸­è‹±æ–‡å¯¹åº”

è¯·ä¸¥æ ¼éµå®ˆè§„åˆ™å›ç­”:"""

        return prompt

    def query(
            self,
            question: str,
            top_k: int = 5,
            show_sources: bool = True,
            temperature: float = 0.3
    ) -> Dict:
        """
        RAGæŸ¥è¯¢(æ”¹è¿›ç‰ˆ)
        """
        print(f"\nâ“ ç”¨æˆ·é—®é¢˜: {question}")
        print("-" * 70)

        # 1. æ£€ç´¢
        print(f"ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£(Top {top_k})...")
        retrieved_docs = self.retrieve(question, top_k)

        if show_sources:
            print(f"\nğŸ“‹ æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(retrieved_docs):
                # â¬‡ï¸ æ˜¾ç¤ºæ­£ç¡®çš„ç›¸ä¼¼åº¦
                print(f"\n  [{i + 1}] ç›¸ä¼¼åº¦: {doc['similarity']:.3f} | è·ç¦»: {doc['distance']:.2f}")
                print(f"      {doc['text'][:200]}...")

        # 2. æ„å»ºprompt
        prompt = self.build_prompt(question, retrieved_docs)

        # 3. ç”Ÿæˆ
        print(f"\nğŸ¤– LLMæ­£åœ¨ç”Ÿæˆç­”æ¡ˆ(temperature={temperature})...")
        answer = self.llm.generate(
            prompt=prompt,
            max_new_tokens=512,
            temperature=temperature,  # â¬…ï¸ æ›´ä¿å®ˆ
            do_sample=True
        )

        print(f"\nğŸ’¡ å›ç­”:\n{answer}")

        return {
            'question': question,
            'answer': answer,
            'sources': retrieved_docs
        }


# ========== æ¼”ç¤ºä»£ç  ==========
def demo():
    """å®Œæ•´æ¼”ç¤º"""
    print("ğŸ¯ æ”¹è¿›ç‰ˆRAGç³»ç»Ÿæ¼”ç¤º\n")

    # 1. åˆå§‹åŒ–
    rag = AdvancedRAGv2()

    # 2. æ·»åŠ çŸ¥è¯†
    pdf_path = "åŸºäºå¤šç­–ç•¥æ”¹è¿›MRFOç®—æ³•çš„å®¶åº­èƒ½æºè°ƒåº¦ä¼˜åŒ– (å·²è‡ªåŠ¨æ¢å¤).pdf"

    if os.path.exists(pdf_path):
        print("\n" + "=" * 70)
        print("ğŸ“š Step 1: æ„å»ºçŸ¥è¯†åº“")
        print("=" * 70)

        num_chunks = rag.add_documents_from_pdf(pdf_path)
        print(f"\nâœ… çŸ¥è¯†åº“å·²åŒ…å« {num_chunks} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°PDF: {pdf_path}")
        return

    # 3. æµ‹è¯•æŸ¥è¯¢
    print("\n" + "=" * 70)
    print("ğŸ§ª Step 2: æµ‹è¯•RAGæŸ¥è¯¢")
    print("=" * 70)

    test_questions = [
        "MRFOç®—æ³•çš„ä¸‰ç§è§…é£Ÿç­–ç•¥æ˜¯ä»€ä¹ˆ?",  # â¬…ï¸ æ›´å…·ä½“çš„é—®é¢˜
        "DLM MRFOç®—æ³•å¼•å…¥äº†å“ªäº›æ”¹è¿›æœºåˆ¶?",
        "åœ¨å¤æ‚åœºæ™¯ä¸‹,DLM MRFOç®—æ³•ç›¸æ¯”MRFOç®—æ³•é™ä½äº†å¤šå°‘æˆæœ¬?",
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'=' * 70}")
        print(f"æµ‹è¯•é—®é¢˜ {i}/{len(test_questions)}")
        print(f"{'=' * 70}")

        result = rag.query(
            question=question,
            top_k=5,  # â¬…ï¸ æ£€ç´¢5ä¸ªæ–‡æ¡£
            show_sources=True,
            temperature=0.3  # â¬…ï¸ æ›´ä¿å®ˆ
        )

        print("\n" + "-" * 70)
        input("æŒ‰Enterç»§ç»­...")

    print("\n" + "=" * 70)
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)


if __name__ == "__main__":
    demo()
