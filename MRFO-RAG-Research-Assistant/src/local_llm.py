"""
æœ¬åœ°LLMå°è£…: æ”¯æŒQwenã€GLMç­‰å¼€æºæ¨¡å‹
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from typing import Optional, List, Dict


class LocalLLM:
    def __init__(
            self,
            model_name: str = "Qwen/Qwen2.5-1.5B-Instruct",
            use_4bit: bool = True,
            device: str = "auto"
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°LLM

        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            use_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–(èŠ‚çœæ˜¾å­˜)
            device: è®¾å¤‡("auto", "cuda", "cpu")
        """
        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹")
        print("=" * 60)
        print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        print(f"âš™ï¸  é‡åŒ–: {'4-bit' if use_4bit else '16-bit'}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
        print()

        self.model_name = model_name

        # é…ç½®é‡åŒ–(é‡è¦!4bité‡åŒ–èƒ½æŠŠæ˜¾å­˜å ç”¨ä»14GBé™åˆ°4GB)
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,  # 4bité‡åŒ–
                bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—ç”¨float16
                bnb_4bit_quant_type="nf4",  # é‡åŒ–ç±»å‹
                bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–
            )
            print("ğŸ”§ ä½¿ç”¨4-bité‡åŒ–é…ç½®")
        else:
            quantization_config = None

        # åŠ è½½Tokenizer
        print("ğŸ”„ åŠ è½½Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print("âœ… TokenizeråŠ è½½å®Œæˆ")

        # åŠ è½½æ¨¡å‹(è¿™ä¸€æ­¥ä¼šä¸‹è½½,ç¬¬ä¸€æ¬¡å¾ˆæ…¢)
        print("\nğŸ”„ åŠ è½½æ¨¡å‹(é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½,çº¦4-7GB,è¯·è€å¿ƒç­‰å¾…)...")
        print("ğŸ’¡ æç¤º: ä¸‹è½½åä¼šç¼“å­˜,ä¸‹æ¬¡å¯åŠ¨å¾ˆå¿«\n")

        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map=device,  # è‡ªåŠ¨åˆ†é…åˆ°GPU
            trust_remote_code=True,
            torch_dtype=torch.float16 if not use_4bit else "auto"
        )

        print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ!")

        # æ˜¾ç¤ºæ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024 ** 3
            memory_reserved = torch.cuda.memory_reserved() / 1024 ** 3
            print(f"ğŸ“Š æ˜¾å­˜å ç”¨: {memory_allocated:.2f} GB (å·²åˆ†é…)")
            print(f"ğŸ“Š æ˜¾å­˜é¢„ç•™: {memory_reserved:.2f} GB (å·²é¢„ç•™)")

        print("=" * 60)
        print()

    def generate(
            self,
            prompt: str,
            max_new_tokens: int = 512,
            temperature: float = 0.7,
            top_p: float = 0.9,
            do_sample: bool = True,
            system_prompt: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆå›ç­”

        Args:
            prompt: ç”¨æˆ·é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦(è¶Šé«˜è¶Šéšæœº,0.1-1.0)
            top_p: æ ¸é‡‡æ ·æ¦‚ç‡
            do_sample: æ˜¯å¦é‡‡æ ·(False=è´ªå©ªè§£ç )
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # æ„å»ºæ¶ˆæ¯(ç¬¦åˆQwençš„chatæ ¼å¼)
        messages = []

        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })

        messages.append({
            "role": "user",
            "content": prompt
        })

        # åº”ç”¨chatæ¨¡æ¿
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize
        model_inputs = self.tokenizer([text], return_tensors="pt")

        # ç§»åˆ°GPU
        if torch.cuda.is_available():
            model_inputs = {k: v.to(self.model.device) for k, v in model_inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç (åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†)
        generated_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(model_inputs["input_ids"], generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return response

    def chat(self, history: List[Dict[str, str]], user_message: str) -> str:
        """
        å¤šè½®å¯¹è¯

        Args:
            history: å¯¹è¯å†å² [{"role": "user", "content": "..."}, ...]
            user_message: æ–°çš„ç”¨æˆ·æ¶ˆæ¯

        Returns:
            åŠ©æ‰‹å›å¤
        """
        # æ·»åŠ æ–°æ¶ˆæ¯
        messages = history + [{"role": "user", "content": user_message}]

        # åº”ç”¨æ¨¡æ¿
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.7
            )

        generated_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(model_inputs["input_ids"], generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

        return response


# ========== æµ‹è¯•ä»£ç  ==========
def test_basic_generation():
    """
    æµ‹è¯•åŸºç¡€ç”Ÿæˆèƒ½åŠ›
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•1: åŸºç¡€é—®ç­”")
    print("=" * 60)

    # åˆå§‹åŒ–æ¨¡å‹(ç¬¬ä¸€æ¬¡ä¼šä¸‹è½½,è¦ç­‰ä¸€ä¼šå„¿)
    llm = LocalLLM(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        use_4bit=True  # ä½¿ç”¨4bité‡åŒ–
    )

    # ç®€å•é—®ç­”
    questions = [
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹?ç”¨ä¸€å¥è¯è§£é‡Šã€‚",
        "Pythonå’ŒJavaæœ‰ä»€ä¹ˆåŒºåˆ«?",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯Attentionæœºåˆ¶ã€‚"
    ]

    for i, question in enumerate(questions, 1):
        print(f"\nâ“ é—®é¢˜{i}: {question}")
        print("-" * 60)

        answer = llm.generate(
            prompt=question,
            max_new_tokens=200,
            temperature=0.7
        )

        print(f"ğŸ’¡ å›ç­”: {answer}")
        print()


def test_with_system_prompt():
    """
    æµ‹è¯•ç³»ç»Ÿæç¤ºè¯
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•2: ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯")
    print("=" * 60)

    llm = LocalLLM(use_4bit=True)

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªMRFOç®—æ³•ä¸“å®¶,ä¸“é—¨ç ”ç©¶å…ƒå¯å‘å¼ä¼˜åŒ–ç®—æ³•ã€‚
è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"""

    question = "MRFOç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆ?"

    print(f"\nğŸ­ ç³»ç»Ÿæç¤º: {system_prompt}")
    print(f"\nâ“ é—®é¢˜: {question}")
    print("-" * 60)

    answer = llm.generate(
        prompt=question,
        system_prompt=system_prompt,
        temperature=0.5  # æ›´ä¸“ä¸š,é™ä½æ¸©åº¦
    )

    print(f"ğŸ’¡ å›ç­”: {answer}")


def test_multi_turn_chat():
    """
    æµ‹è¯•å¤šè½®å¯¹è¯
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•3: å¤šè½®å¯¹è¯")
    print("=" * 60)

    llm = LocalLLM(use_4bit=True)

    # å¯¹è¯å†å²
    history = []

    # ç¬¬ä¸€è½®
    print("\nğŸ‘¤ ç”¨æˆ·: ä»€ä¹ˆæ˜¯RAG?")
    response1 = llm.chat(history, "ä»€ä¹ˆæ˜¯RAG?")
    print(f"ğŸ¤– åŠ©æ‰‹: {response1}")

    history.append({"role": "user", "content": "ä»€ä¹ˆæ˜¯RAG?"})
    history.append({"role": "assistant", "content": response1})

    # ç¬¬äºŒè½®
    print("\nğŸ‘¤ ç”¨æˆ·: å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿?")
    response2 = llm.chat(history, "å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿?")
    print(f"ğŸ¤– åŠ©æ‰‹: {response2}")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸš€ LocalLLM æµ‹è¯•ç¨‹åº")
    print()

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨,å°†ä½¿ç”¨CPU(ä¼šå¾ˆæ…¢)")

    print()

    # è¿è¡Œæµ‹è¯•
    try:
        test_basic_generation()
        test_with_system_prompt()  # å–æ¶ˆæ³¨é‡Šæ¥æµ‹è¯•
        test_multi_turn_chat()      # å–æ¶ˆæ³¨é‡Šæ¥æµ‹è¯•

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
